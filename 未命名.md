# 一.机器学习？深度学习？
### 1.机器学习（Machine Learning）是一种人工智能（AI）的分支，它致力于让计算机系统能够从数据中学习并改进其性能，而不需要明确编程。
从定义不难看出，有了机器学习，我们只需要把数据喂给计算机，计算机就能自动将其加工，然后将加工结果返回给我们。
#####机器学习可以理解为：
#####输入（图像）——>>算法（图像识别）——>>输出（识别的内容）
#####输入（音频）——>>算法（音频识别）——>>输出（识别的内容）
#####等等......
其中，科学家对机器学习的开发主要集中在“算法”这一部分，其中包含了大量的数学模型的参与，因此，机器学习并没有网传的那么神奇，更多的是严谨的计算机算法的不断发展。
###2.深度学习（Deep Learning）是一种机器学习方法，它模仿人脑的神经网络结构，用于解决复杂的任务，如图像识别、自然语言处理、语音识别和推荐系统等。
#####深度学习是机器学习的一种方法
深度学习是机器学习的一种方法，除了深度学习以外，机器学习还有决策树，随机森林等等，后者被称为传统机器学习。
#####深度学习与传统机器学习的区别
深度学习在处理复杂数据，如图像、音频、文本和视频等方面表现更出色，深度学习可以自动提取数据特征。相应的，深度学习需要的数据更加庞大，理解和解释起来也有更大的难度。
传统机器学习则需要人工设计和提取特征，这需要专业知识和时间来完成。同时意味着，传统机器学习更倾向于处理相对较小的数据。
#####我对机器学习和深度学习的理解
深度学习经常更与“大”“复杂”等词挂钩，从感觉上就比传统机器学习更加高级。减少了人工提取特征的过程，更多的工作由电脑来完成，这样的话，不仅节省时间，而且即使不是某方面的专家，也可以尝试处理这方面的数据。
随着社会发展，信息会更加多元，更加复杂，类似于图片音频等数据会更加难以人工处理。信息量增大，人工处理的成本也会越来越大。相比于传统机器学习，深度学习的前景明显更加广阔，应该是机器学习发展的一种趋势。
#####神经网络（Neural Network）是一种算法
神经网络（Neural Network）是一种计算模型，灵感来自生物神经系统的工作原理。有人说深度学习约等于神经网络，因为神经网络是深度学习的基本组成元素，或者说深度学习是依靠神经网络运行起来的。神经网络用来进行数据传递和处理，中间包括很多权重，神经网络可以自动调整权重，让数据处理越来越准确。
_ _ _

#二.监督学习与无监督学习
###1.监督学习（Supervised Learning）是机器学习的一种主要范式，算法通过学习输入数据和相应的标签之间的关联关系，以进行预测和分类。
监督学习的数据有“输入数据”和“标签”，其中学习的是数据与标签之间的联系。这种学习的目标是，计算机要对未知的数据进行预测和分类，举一个分类的例子：
例如：糖尿病病人的检测，我们有若干人的数据，每个人都有数十个指标，有的人患病，有的人不患病，人眼判断不出是否患病与各项身体指标的关系，但机器学习可以。计算机将数据进行学习后，如果我将张三的体检指标输入，它就可以判断张三是否患糖尿病。
###2.无监督学习（Unsupervised Learning）是机器学习的一种范式，与监督学习不同，它的目标是从数据中发现模式、结构或规律，而不需要预先提供标签或目标输出。
不难看出，无监督学习的数据里不包含“标签”，也不需要输出预测或分类。它的主要任务是聚类、降维、密度估计等。
聚类可以理解为把同类数据的放到一起；降维可以理解为提取多个数据的本质特征；密度估计可以理解为估计数据的分布特点。
无监督学习给人一种不需要结果，而需要数据之间关系的感觉，很容易让人想到的例子就是短视频平台根据喜好推送内容的行为。网上对无监督学习的说法是我们无法知道无监督学习会学成什么样子，可能是因为没有标签作为参考，这也体现了“无监督”的特点。而监督学习给人的感觉就是需要一个明确的结果，比如预测值或者分类，像是iris数据集那种。
_ _ _

#三.一些数学概念
###1.偏导数（Partial Derivative）用于描述多元函数中某个变量的变化率。只考虑一个特定的自变量的变化对函数值的影响，而将其他自变量视为常数。
对于一元函数来说，偏导数应该就是导数。于多元函数来说，自变量有x1，x2，...，xn，这时我们不能说y对x的导数，而应该说y对x1（或其他自变量）的偏导数，这时其他自变量当常数看待。
偏导数和导数差不多，是把导数拓展了，每个自变量的偏导数组成了梯度向量，该向量指示了多元函数在特定点的变化方向和速率，类似于一元函数的斜率。
之前说过神经网络可以调整权重，调整权重就需要变化方向和速率，前者确定权重往哪里调整，后者确定调整多少。而权重有很多，所以要用偏导数。
###2.链式法则（Chain Rule）用于计算复合函数的导数。
通过后面的学习，我们知道了在神经网络里，我们要求损失对权重的导数，而计算的时候是输入x->计算估计->计算损失。这可以看成是在计算f(g(x))这样的函数，如果我们要求损失对权重的导数，就需要复合函数求导，即链式法则。
###3.梯度（Gradient）是多元函数的偏导数的集合，用于描述函数在某一点的变化率和方向。
梯度是所有偏导数组成的向量，梯度∇f 在某一点 (x1, x2, ..., xn) 处的值告诉我们，如果要在该点找到函数 f 的最大增长率，应该沿着梯度的方向移动，果要找到最大减小率（即函数的最小值），则应该沿着梯度的相反方向移动。
上面是网上的定义，但很容易理解，因为我们在神经网络里正是要寻找损失最小时的权重，如果把计算损失当成函数的话，我们要找的就是这个函数的最小值点，也就是沿着梯度的相反方向移动。
###4.矩阵乘法
矩阵乘法正好大一学，乘积的某个元是前一个矩阵的对应行的元和后一个矩阵对应列的元相乘之后再相加。两个矩阵相乘的前提是前一个矩阵的列数等于后一个矩阵的行数。
神经网络里要用到矩阵乘法，因为输入数据可能有多个特征，输出数据也有我们规定的数量，想要完成这种计算，中间的权重必须是一个矩阵。
如果输入有5个特征，输出有7个特征，权重就需要是一个7乘5的矩阵
_ _ _

#四.一些重要概念
###1.损失函数（Loss Function）用于度量模型的性能，即模型的预测值与实际观察值之间的差异。
前面说过，我们需要计算损失，这就需要损失函数。最开始学线性回归的时候，我们用的是MSE损失函数，也就是简单的进行（y-y_pred）^2。
后来又学习了BCE函数，是用来计算二分类问题的损失，BCE(y, p) = - [y * log(p) + (1 - y) * log(1 - p)]
再后来涉及多分类问题，多分类问题输出的是概率表，概率表不能进行相减后去平方的那种求损失方法，在数学界求概率分布损失的方法有交叉熵等等，虽然不直道具体是怎么求的，但是我们知道引用crossentropy损失函数来求损失。
###2.梯度下降（Gradient Descent）是一种用于优化目标函数的迭代优化算法。
前面讲到梯度用来描述函数在某一点的变化方向和大小，如果我们要找到损失最小的权重，就需要用梯度下降算法，不断更新权重。
#####可以这么理解：
#####已知损失函数——>>计算损失——>>计算梯度——>>更新参数——>>重复多次直到收敛
其中，更新参数这一步是θ = θ - η * ∇J(θ)，η是学习率，人为设置，∇J(θ)是计算的梯度
学习率一般为正，梯度是目标函数在当前参数值附近的最陡峭上升方向，也就是说函数沿着梯度的方向上升最快，但我们想要的是损失减小，所以用减号让参数θ沿梯度的负方向变化。然后，随着参数的变化，梯度也会随之下降。
#####梯度下降的三个变种
批量梯度下降：每次迭代使用整个训练数据集来计算梯度。缺点是数据太多，进行太慢。
随机梯度下降：每次迭代使用单个或一小批样本来计算梯度，缺点是有一些特殊的值，被称作“嘈杂“。
小批量梯度下降：也就是我们学的minibatch，综合了前两者的优点。
###3.反向传播（Backpropagation）的主要目标是计算模型预测与实际标签之间的误差，将这个误差反向传播到网络中的每一层，以调整网络的权重和偏差。
简单点来说，我们需要进行梯度下降，必须要知道梯度，反向传播就是将梯度计算出来，然后返回到计算它的每一层，可以理解为“从哪里来回哪里去”，当梯度忘前传播，前面的层也计算对应的梯度，一直传到含有权重的那一层，权重根据传播过来的梯度进行梯度下降。
_ _ _
#五.样本，特征，激活函数
###样本（sample）是指从总体或总体群体中抽取的一组观测或数据点的集合。
###特征（Feature）通常是指用来描述数据或表示数据的属性、属性值、变量或观测值。
不太好解释，举个例子：
我有100个疑似糖尿病患者的体检报告，每份体检报告上有包括血糖浓度，血蛋白浓度在内的10个指标。这100个人的数据就是100个样本，血糖浓度，血蛋白浓度等指标就是10个特征，一般情况下，样本可以有很多，但特征的数量是固定不变的。比如我可以再找200个人，就是加入了200个新样本，但是特征里面不会出现新的特征。
###激活函数
从网上的定义来看，如果没有激活函数，神经网络就只剩线性变换，很明显，如果只有线性变换，无论变换多少次，最终都只能处理线性问题。而激活函数有非线性性，引入激活函数可以处理非线性问题，虽然我还不是很理解激活函数是怎么引进非线性性的，但既然它有这个性质，分类问题就是非线性的，用激活函数就可以处理分类问题。

_ _ _
#六.线性回归和逻辑回归
###线性回归是一种用于解决回归问题的统计方法，它建立了自变量（特征）和因变量（目标）之间的线性关系。
线性回归的基本原理是构建一个线性方程，特征值与权重进行计算后相加（可能还要加偏置量）得到输出值，在训练时，先构建线性方程，经过梯度下降算法不断更新权重，最后得到损失收敛时的权重。然后，当我们输入新的特征时，就可以根据已经训练得到的权重计算特征，得到输出。

###逻辑回归是一种用于解决二元分类问题的方法，它可以估计观测值属于两个类别中的一个的概率。
虽然名叫回归，但逻辑回归是解决分类问题的。逻辑回归的基本原理是构建一个逻辑函数，用于将线性组合的特征映射到0到1的范围内。在我看来，逻辑回归的标签只有0和1两种，我们对特征做逻辑变换后得到的值，从某种程度上就能表明输出结果更接近1还是更接近0，我们只需要想办法让输出结果和更接近谁联系起来，而这种联系就是把他映射到0和1之间，用映射值作为输出结果是1的概率。
如果映射接近1，我们就认为分类是1，如果接近0，我们就认为分类是0。
###线性回归与逻辑回归的联系与区别
#####联系：
不难看出，在进行逻辑回归时我们也用到了线性回归。
#####区别：
线性回归输出的是预测值，而逻辑回归只能输出0到1之间的概率，线性回归可以输出任何数。
线性回归计算的是实际值与预测值之间的误差，属于实数误差，逻辑回归计算的是概率分布之间的误差。
线性回归用于预测，逻辑回归用于分类。

_ _ _
#七.数据集的划分
###1.各集的作用
#####训练集：
训练集占大部分，用来训练模型，一般占70%左右
#####验证集：
评估模型性能，如果模型在训练集上表现良好但在测试集上准确率低，说明存在泛化性能差的问题，可能是过拟合（训练过头了）或者欠拟合（训练浅了）的问题
可以调整超参数或者选择学习器
为什么不在测试集进行以上步骤？这就是第三点原因，不能泄露测试集的信息，不然可能导致模型与测试集数据过于拟合导致准确率偏高
#####测试集：
评估模型可靠性
#####三者的关系：
训练集进行训练，验证集来验证训练结果，对问题（过拟合，超参数的选择，学习器的选择）进行调整，测试集来评估训练的成果如何。
###2.正确划分数据集
#####不正确划分的影响
过拟合：训练集太小，无法学习足够的特征模式
欠拟合：训练集太小也有可能欠拟合，因为模型可能没办法捕捉到数据中的重要信息
超参数调优偏差：验证集太小，导致选择了不当的超参数
模型选择偏差：验证集太小，原因同上
性能评估偏差：测试集太小或者不具有代表性
#####正确的数据划分：典型的划分比例可以是 70-80% 的数据用于训练，10-15% 用于验证，10-15% 用于测试。

_ _ _
#八.softmax函数
###Softmax函数是一种常用于多类别分类问题的激活函数，通常用于神经网络的输出层。
###softmax函数的主要作用是将一个实数向量转化为表示概率分布的向量，其中每个元素表示一个类别的概率。
pi = e^zi / (e^z1 + e^z2 + ... + e^zn)
即对每一个输出值取对数后相加，每个值的对数除以对数的和输出。
#####softmax函数的作用：
保证概率之和为1
保证概率均大于0








